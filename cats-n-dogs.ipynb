{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kg config -c dogs-vs-cats-redux-kernels-edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): www.kaggle.com\n",
      "downloading https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/test.zip\n",
      "\n",
      "Starting new HTTPS connection (1): storage.googleapis.com\n",
      "test.zip already downloaded !\n",
      "downloading https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/train.zip\n",
      "\n",
      "train.zip already downloaded !\n",
      "downloading https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/sample_submission.csv\n",
      "\n",
      "sample_submission.csv already downloaded !\n"
     ]
    }
   ],
   "source": [
    "!kg download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "# These commands consistently crashed my notebook...? Whatevs.\n",
    "# unzip test.zip\n",
    "# unzip train.zip\n",
    "!ls train | wc -l\n",
    "!ls test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing train data\n",
      "Organizing test data\n",
      "Creating validation data from training data\n",
      "Copying sample train, validation, and test data\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Organizing train data\"\n",
    "mkdir -p train/cats train/dogs\n",
    "find train/dog* -maxdepth 0 -type f | xargs -I {} mv {} train/dogs\n",
    "find train/cat* -maxdepth 0 -type f | xargs -I {} mv {} train/cats\n",
    "\n",
    "echo \"Organizing test data\"\n",
    "mkdir -p test/unknown\n",
    "find test -maxdepth 0 -type f | xargs -I {} mv {} test/unknown\n",
    "\n",
    "echo \"Creating validation data from training data\"\n",
    "rm -rf valid\n",
    "mkdir -p valid/cats valid/dogs\n",
    "find train/dogs -type f | shuf -n 1000 | xargs -I {} mv {} valid/dogs\n",
    "find train/cats -type f | shuf -n 1000 | xargs -I {} mv {} valid/cats\n",
    "\n",
    "echo \"Copying sample train, validation, and test data\"\n",
    "rm -rf sample\n",
    "\n",
    "mkdir -p sample/train/cats sample/train/dogs\n",
    "find train/dogs -type f | shuf -n 8 | xargs -I {} cp {} sample/train/dogs\n",
    "find train/cats -type f | shuf -n 8 | xargs -I {} cp {} sample/train/cats\n",
    "\n",
    "mkdir -p sample/valid/cats sample/valid/dogs\n",
    "find train/dogs -type f | shuf -n 4 | xargs -I {} cp {} sample/valid/dogs\n",
    "find train/cats -type f | shuf -n 4 | xargs -I {} cp {} sample/valid/cats\n",
    "\n",
    "mkdir -p sample/test/unknown\n",
    "find test/unknown -type f | shuf -n 8 | xargs -I {} cp {} sample/test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23003\n",
      "2003\n",
      "12502\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "!find train | wc -l\n",
    "!find valid | wc -l\n",
    "!find test | wc -l\n",
    "!find sample | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a bunch of useful Python files in the class repo - copy them to this working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ~/courses/deeplearning1/nbs/*.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toggle this cell to switch between real data or very small amounts of data, which is good for iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "# path = \"sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and fit model\n",
    "\n",
    "The loss function is calculated with categorial cross-entropy. `loss` is subject to dropout, `val_loss` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has some neat Callbacks features, but the `fit` method defined in vgg16.py doesn't allow me to pass them into the underlying Keras function. Next week when we just write our own model directly in Keras this won't be a problem, but this time I figured I'd just monkey-patch the vgg instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self, batches, val_batches, nb_epoch=1, callbacks=None):\n",
    "    \"\"\"\n",
    "        Fits the model on data yielded batch-by-batch by a Python generator.\n",
    "        See Keras documentation: https://keras.io/models/model/\n",
    "    \"\"\"\n",
    "    self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n",
    "            validation_data=val_batches, nb_val_samples=val_batches.nb_sample, callbacks=callbacks)\n",
    "\n",
    "# This monkey patching technique binds `self` properly\n",
    "# (vgg.fit = fit` does not - you'd have to pass `self=vgg` to invoke properly).\n",
    "\n",
    "import types\n",
    "vgg.fit = types.MethodType(fit, vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Interrupt training when validation loss stops decreasing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Save the weights and architecture of the model after the best epoch (by validation loss)\n",
    "checkpointer = ModelCheckpoint(\"best-weights.hdf5\", monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 3904/23000 [====>.........................] - ETA: 484s - loss: 0.2260 - acc: 0.9326"
     ]
    }
   ],
   "source": [
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, val_batches, 50, callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batches = vgg.get_batches(path+'test', batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of batches we'll iterate through.\n",
    "\n",
    "Check out that python value being interpolated into a bash command and then saved into another variable! Holy shit, that's awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "command_output = !ls {path + 'test/unknown'} | wc -l\n",
    "num_test_files = int(command_output[0])\n",
    "print(num_test_files)\n",
    "num_batches = int(ceil(num_test_files / batch_size))\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* `re.match` immediately returns no match if the beginning of the string doesn't match :|. `re.search` will find a match anywhere in the string.\n",
    "* `batches.filenames` is key to identifying the filename of each prediction.\n",
    "* As Jeremy points out at the beginning of Lesson 2, being overly confident really hurts your log loss score (https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition#evaluation). Clip 0s and 1s to be 5% less confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('results.csv', 'w') as csvfile:\n",
    "    fieldnames = ['id', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for batch_index in log_progress(range(0, num_batches)):\n",
    "        images, labels = next(batches)\n",
    "        confidences, predictions, predictions_english = vgg.predict(images)\n",
    "        \n",
    "        # Being overly confident really hurts your log loss score https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition#evaluation\n",
    "        predictions = np.clip(predictions, 0.05, 0.95)\n",
    "        \n",
    "        # plots(images, titles=predictions)  # display pictures with predictions, as a sanity check.\n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            # extract image ID from filename\n",
    "            index = batch_index * batch_size + i\n",
    "            filename = re.search(\"\\d+\", batches.filenames[index]).group(0)\n",
    "            prediction = predictions[i]\n",
    "            writer.writerow({'id': filename, 'label': round(prediction, 2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date = datetime.now().strftime(\"%H:%M %m/%d/%Y\")\n",
    "!kg submit results.csv -m \"Submitted {date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
